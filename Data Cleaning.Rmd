## Read in Our Data

We first load in the dataset from Kaggle, the train.csv file. By nature of this data, we only will use the train set for our analysis, and for sake of computational power, we only use 100000 rows of the over 300000 available. Otherwise, our models would take far too long to fit. This allows us to still have sufficient data in both our first and second level, train and test sets alike.
```{r}
mortgage <- read.csv("application_train.csv", stringsAsFactors = TRUE)
set.seed(12345)
rows <- sample(1:nrow(mortgage), 100000)

dat <- mortgage[rows, ]

str(dat)
summary(dat)
```

## Clean Data

Eliminating columns that do not add a lot of additional information. We found these through our initial data exploration on Kaggle. These variables are almost all missing, which makes it very difficult to use them in the model-building process.

```{r}
dat$FLAG_OWN_CAR <- NULL
dat$FLAG_OWN_REALTY <- NULL
dat$FLAG_MOBIL <- NULL
dat$FLAG_CONT_MOBILE <- NULL
dat$FLAG_DOCUMENT_2 <- NULL
dat$FLAG_DOCUMENT_4 <- NULL
dat$FLAG_DOCUMENT_7 <- NULL
dat$FLAG_DOCUMENT_9 <- NULL
dat$FLAG_DOCUMENT_10 <- NULL
dat$FLAG_DOCUMENT_11 <- NULL
dat$FLAG_DOCUMENT_12 <- NULL
dat$FLAG_DOCUMENT_13 <- NULL
dat$FLAG_DOCUMENT_14 <- NULL
dat$FLAG_DOCUMENT_15 <- NULL
dat$FLAG_DOCUMENT_16 <- NULL
dat$FLAG_DOCUMENT_17 <- NULL
dat$FLAG_DOCUMENT_18 <- NULL
dat$FLAG_DOCUMENT_19 <- NULL
dat$FLAG_DOCUMENT_20 <- NULL
dat$FLAG_DOCUMENT_21 <- NULL
dat$SK_ID_CURR <- NULL
```

Filling in NA data with the mean of the column, created dummy variables, and scaled the data. We also remove any columns which do not have unique values, otherwise some of our models will not be able to run. This is due to the singular columns made in a matrix during computation. 

```{r}
dat[sapply(dat, is.numeric)] <- lapply(dat[sapply(dat, is.numeric)], function(x) ifelse(is.na(x), mean(x, na.rm = TRUE), x))

dat_d <- as.data.frame(model.matrix(~ . -1, data = dat))

minmax<- function(x){
  (x-min(x))/(max(x)-min(x))
}

dat_s <- as.data.frame(lapply(dat_d, minmax))

colSums(is.na(dat_s))
sapply(dat_s, function(x) length(unique(x)))
dat_s <- dat_s[ , sapply(dat_s, function(x) length(unique(x)) > 1) ]

```

Split data 50/50 initially to create a train set and a test set. Further down the line, we will split this test data into train and test again, for our second layer model. With 100000 rows, we will have adequate amounts of data in all levels of our model.
```{r}
trainprop <- 0.5
set.seed(12345)
train_rows <- sample(1:nrow(dat_s), trainprop*nrow(dat_s))

default_train <- dat_s[train_rows, ]
default_test <- dat_s[-train_rows, ]

cat("Train\n")
summary(default_train$TARGET)
cat("Test\n")
summary(default_test$TARGET)
cat("Population\n")
summary(dat_s$TARGET)

cat("We see that the means are all similar, showing our splits are representative of the population/n")
```